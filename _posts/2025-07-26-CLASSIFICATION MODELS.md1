---
title: classification model
categories: [matchine learning]

tags : classification models, matchine learning


---
# Import required libraries
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# Load the dataset and convert to DataFrame
wine = load_wine()
X = pd.DataFrame(wine.data, columns=wine.feature_names)
y = pd.Series(wine.target, name='target')
# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# DataFrame to store results
results = pd.DataFrame(columns=['Model', 'Accuracy'])
print(results)

# Helper function for confusion matrix plotting
def plot_conf_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=wine.target_names, yticklabels=wine.target_names)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix: {title}')
    plt.show()

# Train & evaluate models
## 1. Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("Logistic Regression\n", classification_report(y_test, y_pred_lr))
plot_conf_matrix(y_test, y_pred_lr, "Logistic Regression")
results.loc[len(results)] = ['Logistic Regression', accuracy_score(y_test, y_pred_lr)]

# Task
Explain the provided Python code for classifying the Wine dataset, fix any errors, complete the code to train and evaluate multiple classification models (Logistic Regression, Decision Tree, Random Forest, K-Nearest Neighbors, Naive Bayes, and Support Vector Machine), and generate a report in markdown format with sections for Introduction, Task Completion (including screenshots of data loading/exploration, model training, and evaluation results), and Conclusion.

## Fix code

### Subtask:
Address the `NameError` by adding a step to scale the features before splitting the data.


**Reasoning**:
The previous cell failed because `X_scaled` was not defined. I need to add a step to scale the features before splitting the data into training and testing sets. This involves importing `StandardScaler`, instantiating it, and then fitting and transforming the data.



from sklearn.preprocessing import StandardScaler

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert scaled data back to a DataFrame (optional but good practice for clarity)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# DataFrame to store results
results = pd.DataFrame(columns=['Model', 'Accuracy'])

# Helper function for confusion matrix plotting
def plot_conf_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=wine.target_names, yticklabels=wine.target_names)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix: {title}')
    plt.show()

# Train & evaluate models
## 1. Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("Logistic Regression\n", classification_report(y_test, y_pred_lr))
plot_conf_matrix(y_test, y_pred_lr, "Logistic Regression")
results.loc[len(results)] = ['Logistic Regression', accuracy_score(y_test, y_pred_lr)]

**Reasoning**:
The previous code block failed because `X` and `y` were not defined. I need to include the code from the original cell that loads the wine dataset and creates the `X` and `y` dataframes, then add the scaling step, and finally the train/test split and the rest of the model training code.



**Reasoning**:
Continue training and evaluating the remaining models as instructed in the subtask.



## 2. Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
print("Decision Tree\n", classification_report(y_test, y_pred_dt))
plot_conf_matrix(y_test, y_pred_dt, "Decision Tree")
results.loc[len(results)] = ['Decision Tree', accuracy_score(y_test, y_pred_dt)]

## 3. Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest\n", classification_report(y_test, y_pred_rf))
plot_conf_matrix(y_test, y_pred_rf, "Random Forest")
results.loc[len(results)] = ['Random Forest', accuracy_score(y_test, y_pred_rf)]

## 4. K-Nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
print("K-Nearest Neighbors\n", classification_report(y_test, y_pred_knn))
plot_conf_matrix(y_test, y_pred_knn, "K-Nearest Neighbors")
results.loc[len(results)] = ['K-Nearest Neighbors', accuracy_score(y_test, y_pred_knn)]

## 5. Gaussian Naive Bayes
nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_test)
print("Gaussian Naive Bayes\n", classification_report(y_test, y_pred_nb))
plot_conf_matrix(y_test, y_pred_nb, "Gaussian Naive Bayes")
results.loc[len(results)] = ['Gaussian Naive Bayes', accuracy_score(y_test, y_pred_nb)]

## 6. Support Vector Machine
svm = SVC(random_state=42)
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)
print("Support Vector Machine\n", classification_report(y_test, y_pred_svm))
plot_conf_matrix(y_test, y_pred_svm, "Support Vector Machine")
results.loc[len(results)] = ['Support Vector Machine', accuracy_score(y_test, y_pred_svm)]